# Model Evaluation: mistralai/Mistral-7B-Instruct-v0.3

**Date:** 2025-11-29
**Test Set:** 21 queries

## Results Summary

| Method | Accuracy | Execution Errors | Notes |
|--------|----------|------------------|-------|
| Smart MAGIC | 9/21 (42.9%) | 4 | |
| Smart MAGIC + Guidelines | 10/21 (47.6%) | 1 | |
| Smart MAGIC + Retry | 9/21 (42.9%) | 3 | |

## Comparison with Llama 3.1 8B

| Method | Llama 3.1 8B | Mistral-7B-Instruct-v0.3 | Difference |
|--------|--------------|-------------------------|------------|
| Smart MAGIC | 52.4% | 42.9% | -9.5pp |
| Smart MAGIC + Guidelines | 57.1% | 47.6% | -9.5pp |
| Smart MAGIC + Retry | 47.6% | 42.9% | -4.7pp |

## FinSQL Results

*To be evaluated*

