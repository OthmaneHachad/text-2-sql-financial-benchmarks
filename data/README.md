# Data Directory

This directory contains training/test datasets and evaluation results for Text-to-SQL experiments.

## Structure

```
data/
├── train/
│   └── queries.json              # 203 training examples
├── test/
│   └── queries.json              # 21 test examples
├── verified_queries.json         # All 224 queries with ground truth
├── results/                      # Evaluation results (not tracked)
│   ├── magic/                    # MAGIC variant results
│   ├── enhanced_magic/           # Enhanced MAGIC results
│   ├── magic_baseline/           # MAGIC Baseline results
│   └── finsql/                   # FinSQL results
├── final_guideline.txt           # Trained MAGIC guidelines (not tracked)
└── guideline_*.txt               # Intermediate guidelines (not tracked)
```

---

## Dataset Files

### train/queries.json

**203 training examples** used for:
- Training FinSQL schema linker (BERT model)
- Generating MAGIC guidelines
- Fine-tuning LoRA adapters

**Format**:
```json
[
  {
    "id": 1,
    "question": "List all available sectors in the GFS data",
    "ground_truth_sql": "SELECT DISTINCT sector_name FROM sectors",
    "difficulty": "simple",
    "tables_used": ["sectors"]
  },
  ...
]
```

### test/queries.json

**21 test examples** used for evaluation:
- 6 simple queries (1-table, basic SELECT)
- 11 medium queries (2-3 tables, JOINs, aggregations)
- 4 hard queries (complex JOINs, subqueries, CTEs)

**Format**: Same as train/queries.json

### verified_queries.json

**All 224 queries** (203 train + 21 test) with verified ground truth SQL.

---

## Results Directory

Results are saved in JSON format but **not tracked in git** (excluded by .gitignore).

### results/magic/

MAGIC variant results for all 5 models:
```
meta_llama_Meta_Llama_3.1_8B_Instruct_Turbo_20251206_123456.json
meta_llama_Meta_Llama_3.1_70B_Instruct_Turbo_20251206_123456.json
...
```

Each file contains:
```json
{
  "model": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
  "method": "Smart MAGIC + Guidelines",
  "timestamp": "20251206_123456",
  "correct": 12,
  "total": 21,
  "accuracy": 57.1,
  "execution_errors": 2,
  "results": [
    {
      "id": 1,
      "correct": true,
      "execution_success": true,
      "predicted_sql": "SELECT ...",
      "ground_truth_sql": "SELECT ...",
      "prompt_tokens": 3200,
      "completion_tokens": 85
    },
    ...
  ]
}
```

### results/enhanced_magic/

Enhanced MAGIC results (schema linking + top-3 guidelines + voting):
- Contains 10-sample voting details
- Higher token usage due to multiple samples

### results/magic_baseline/

MAGIC Baseline results (full schema + 11 guidelines):
- No schema linking
- No voting
- Single sample generation

### results/finsql/

FinSQL results (schema linking + LoRA fine-tuning):
- Only Llama 8B and 70B
- Includes schema linker predictions
- Includes LoRA training metrics

---

## Guideline Files

Generated by `magic/train_magic.py`, **not tracked in git**.

### final_guideline.txt

11 generic SQL generation guidelines extracted from training data:
```
SQL Generation Guidelines:
1. Use exact column names from schema (case-sensitive)
2. Verify table exists before writing JOIN clause
...
11. Verify foreign key relationships before joining tables
```

### guideline_*.txt

Intermediate guidelines generated during training (one per training example).

---

## Regenerating Data

### Regenerate Guidelines

```bash
cd magic
python train_magic.py
```

**Duration**: ~30-45 minutes
**Output**: `data/final_guideline.txt`

### Regenerate Results

Run evaluation scripts:
```bash
# MAGIC variants
cd enhanced_magic
python evaluate_all_methods.py meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo

# FinSQL
cd finsql
python full_finsql_inference.py meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo

# Ablation study
cd enhanced_magic
python run_all_ablations.py
```

---

## Query Format Details

### Required Fields

- `id` (integer): Unique query identifier
- `question` (string): Natural language question
- `ground_truth_sql` (string): Correct SQL query

### Optional Fields

- `difficulty` (string): "simple", "medium", or "hard"
- `tables_used` (list): Tables required for query
- `query_type` (string): "selection", "join", "aggregation", etc.

### Difficulty Definitions

**Simple** (6 queries):
- Single table
- Basic SELECT with WHERE
- No JOINs or aggregations

**Medium** (11 queries):
- 2-3 tables
- JOINs required
- Basic aggregations (COUNT, SUM, AVG)
- GROUP BY clauses

**Hard** (4 queries):
- 3+ tables
- Complex JOINs
- Nested subqueries or CTEs
- Window functions or complex aggregations

---

## Data Statistics

### Training Set (203 queries)
- Simple: ~60 queries (30%)
- Medium: ~110 queries (54%)
- Hard: ~33 queries (16%)

### Test Set (21 queries)
- Simple: 6 queries (29%)
- Medium: 11 queries (52%)
- Hard: 4 queries (19%)

**Distribution**: Test set reflects training distribution.

---

## Notes

- **Results not tracked**: All JSON files in `results/` excluded from git
- **Guidelines not tracked**: `final_guideline.txt` and `guideline_*.txt` excluded
- **Train/test split**: Fixed split, not randomized
- **Ground truth verified**: All queries manually verified against database
- **Reproducible**: Same train/test split used across all experiments

---

**Last Updated**: December 2025
